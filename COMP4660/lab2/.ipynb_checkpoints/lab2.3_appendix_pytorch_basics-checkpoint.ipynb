{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP4660/8420 Lab 2.3 Appendix - Introduction to PyTorch Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before delving deeper into the building neural networks with PyTorch, it is helpful to know some basic manipulations of PyTorch elements. The following examples in task 1 are designed to give you a very brief introduction to operations in PyTorch and will be sufficient for the course, but if you are interested in increasing your knowledge we encourage you to look at the PyTorch tutorials and play around further:\n",
    "http://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a Python based scientific computing package targeted at two sets of audiences:\n",
    "* A replacement for NumPy to use the power of GPUs\n",
    "* a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "To use PyTorch, you need to import the library by writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors\n",
    "\n",
    "### 1.1 What are Tensors?\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be\n",
    "used on a GPU to accelerate computing.\n",
    "\n",
    "Construct a randomly initialized matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a randomly initialized matrix\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "# get matrix size\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tensors Operations\n",
    "\n",
    "Tensors support basic operations such as addition, subtraction, multiplication, and division. There are multiply syntaxes for operations. In the following example, we will take a look at the addition operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Addition: syntax 1\n",
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Addition: syntax 2\n",
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Addition: in-place\n",
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(y)\n",
    "y = y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 1: Any operation that mutates a tensor in-place is post-fixed with an \\_. For example: x.copy\\_(y) , x.t\\_(), will change x.\n",
    "\n",
    "Indexing is also NumPy-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# access the second column\n",
    "print(x[:, 1])\n",
    "\n",
    "# access the first row\n",
    "print(x[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tensors <-> NumPy arrays\n",
    "\n",
    "Tensors can be converted to NumPy’s ndarrays,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy library\n",
    "import numpy as np\n",
    "# create a randomly initialized tensor matrix\n",
    "x = torch.rand(5, 3)\n",
    "# convert tensors to numpy array\n",
    "y = x.numpy()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and can be formed by NumPy’s ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a numpy array\n",
    "x = np.array(([3,4], [3,5]))\n",
    "# convert numpy array to tensor\n",
    "y = torch.from_numpy(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 2: The Torch Tensor and NumPy array will share their underlying memory locations, so changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a numpy array\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "# all elements in a add 1\n",
    "a.add_(1)\n",
    "# b will also be updated\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd: automatic differentiation\n",
    "\n",
    "Central to all neural networks in PyTorch is the _autograd_ package. It provides automatic differentiation for all operations on Tensors.\n",
    "\n",
    "Autograd is reverse automatic differentiation system. Conceptually, _autograd_ records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input variables and roots are the output variables. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "Internally, _autograd_ represents this graph as a graph of Function objects (really expressions), which can be _apply()_ ed to compute the result of evaluating the graph. When computing the forwards pass, _autograd_ simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the .grad\\_fn attribute of each _Variable_ is an entry point into this graph). When the forwards pass is completed, we evaluate this graph in the backwards pass to compute the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Variable (Deprecated)\n",
    "\n",
    "This class has been deprecated as of version 0.4. All functionalities of the class Variable are now supported directly by the class Tensor. The below section is useful to understand what Variable does (if you are using an old PyTorch version) and what Tensor can now do (the functions are directly applicable to Tensor now).\n",
    "\n",
    "autograd.Variables is the central class of the package. It wraps a Tensor, and supports nearly all operations defined on it. Once you finish your computation, you can call .backward() and have all the gradients computed automatically.\n",
    "\n",
    "There are three basic attributes in a Variable:\n",
    "\n",
    "![Variable](images/variable.png)\n",
    "\n",
    "You can access the raw tensor through the .data attribute, the gradient with respect to this variable is accumulated into .grad. Each variable has a .grad\\_fn attribute which references a Function that has created the Variable, except for Variables created by the user – their grad\\_fn is None.\n",
    "\n",
    "If you want to compute the derivatives, you can call .backward() on a Variable. If Variable is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Create variables\n",
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "\n",
    "# Define a function\n",
    "y = w * x + b        \t# y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()         \t# equal to y.backward(torch.Tensor([1.0]))\n",
    "\n",
    "# Print out the gradients.\n",
    "print('dy/dx: {}'.format(x.grad.data))   # x.grad = 2\n",
    "print('dy/dw: {}'.format(w.grad.data))   # w.grad = 1\n",
    "print('dy/db: {}'.format(b.grad.data))   # b.grad = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradients\n",
    "\n",
    "Let’s now look at the following example to understand how gradients are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x=torch.Tensor([[1.,2.,3.],[4.,5.,6.]])\n",
    "x=Variable(x,requires_grad=True)\n",
    "y=x+2\n",
    "z=y*y*3\n",
    "out=z.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent computation graph of the above code is:\n",
    "![Computation Graph](images/graph.png)\n",
    "\n",
    "Now, if you follow the computation direction by using its .grad\\_fn attribute, i.e. by printing out the .grad\\_fn of each variable as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x.grad_fn)\n",
    "print(y.grad_fn)\n",
    "print(z.grad_fn)\n",
    "print(out.grad_fn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will see a graph of computations that looks like this:\n",
    "\n",
    "    x -> add -> multiply -> mean -> out\n",
    "    \n",
    "To see the gradient of out with respect to x, ∂out/∂x, we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.backward()      # equivalent to out.backward(torch.Tensor([1]))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Networks\n",
    "\n",
    "Neural networks can be constructed using the torch.nn packages. nn depends on autograd to define models and differentiate them. An nn.Module contains layers, and a method forward(input) that returns the output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "* Define the neural network that has some learnable parameters (or weights)\n",
    "* Iterate over a dataset of inputs\n",
    "* Process input through the network\n",
    "* Compute the loss (how far is the output from being correct)\n",
    "* Propagate gradients back into the network’s parameters\n",
    "* Update the weights of the network, typically using a simple update rule: \n",
    "    \n",
    "        weight = weight - learning_rate * gradient \n",
    "\n",
    "### 3.1 Define the network\n",
    "A template for defining a neural network is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# a template for defining a neural network\n",
    "Class net_name(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_name, self).__init()\n",
    "        # add layers here\n",
    "        self.layer1 = nn.Linear(n_input, n_hidden)  #change nn.Linear if it is not linear \n",
    "        self.layer2 = …\n",
    "        # more layers…\n",
    "\n",
    "    # define the process of performing forward pass,\n",
    "    # that is how to return a Variable of output data\n",
    "    # from a Variable of input data x\n",
    "    def forward(self, x):\n",
    "        x = F.some_function1(x)        # calling some functions in torch.nn.functional\n",
    "        x = F.some_function2(x)        # calling some functions in torch.nn.functional \n",
    "        x = self.layer1(x)             # apply pre-define layer1 \n",
    "        … …\n",
    "        return x\n",
    "\n",
    "# define a neural network using the customised structure\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just need to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function. The input to the forward is an autograd.Variable, and so is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform forward pass to get the actual output\n",
    "output = net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learnable parameters of a model are returned by net.parameters().\n",
    "\n",
    "Here as an example, we define a simple neural network with one hidden layer using the above template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a simple neural network with one sigmoid hidden layer\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define linear hidden layer output\n",
    "        self.hidden = torch.nn.Linear(n_input, n_hidden)\n",
    "        # define linear output layer output\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            In the forward function we define the process of performing\n",
    "            forward pass, that is to accept a Variable of input\n",
    "            data, x, and return a Variable of output data, y_pred.\n",
    "        \"\"\"\n",
    "        # get hidden layer input\n",
    "        h_input = self.hidden(x)\n",
    "        # define activation function for hidden layer\n",
    "        h_output = F.sigmoid(h_input)\n",
    "        # get output layer output\n",
    "        y_pred = self.out(h_output)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "# define a neural network using the customised structure\n",
    "net = TwoLayerNet(input_neurons, hidden_neurons, output_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Loss Function\n",
    "\n",
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different loss functions under the nn package . A simple loss is: nn.MSELoss which computes the mean-squared error between the input and the target. \n",
    "\n",
    "For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform forward pass to get the actual output\n",
    "output = net(input)\n",
    "\n",
    "# define loss function\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "# compute loss\n",
    "loss = loss_func (output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when we call loss.backward(), the whole computational graph is differentiated with respect to the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Back propagation\n",
    "\n",
    "To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear gradient buffers of all parameters\n",
    "net.zero_grad()\n",
    "\n",
    "# perform backward pass: compute gradients of the loss with respect to\n",
    "# all the learnable parameters of the model.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Update the weights\n",
    "\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "    \n",
    "    weight = weight - learning_rate * gradient\n",
    "    \n",
    "We can implement this using simple python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you may want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we can use a small package torch.optim that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = loss_func(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Save and load a model\n",
    "\n",
    "Sometimes, you may want to save the trained model and load it later. There are two approaches for this.\n",
    "\n",
    "The first (recommended) saves and loads only the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(the_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_model = TheModelClass(*args, **kwargs)\n",
    "the_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second saves and loads the entire model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(the_model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_model = torch.load(PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
