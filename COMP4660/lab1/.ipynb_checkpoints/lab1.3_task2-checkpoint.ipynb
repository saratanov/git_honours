{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP4660/8420 Lab 1.3 - Building a NN in PyTorch using custom Modules\n",
    "\n",
    "While the nn package is useful for quickly building up a neural network, the real power comes when you can specify models that are more complex than a sequence of existing Modules. In order words, you can perform in-depth customization on your neural networks, such as adding multiple hidden layers of neurons, changing the activation functions or changing the learning algorithm. As you will be expected to write your own code for assignments, we will now introduce you to building a neural network with customised nn modules. \n",
    "\n",
    "The python script is an alternative implementation of Task1, but it demonstrates a way to define your own neural network by subclassing nn.Module and defining a forward function which receives input Variables and produces output Variables.  \n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Download and import all required libraries\n",
    "\n",
    "To run this notebook, you need to have the following packages installed:\n",
    "-  torch: a python deep learning package\n",
    "-  pandas: a python data analysis package; if you are familiar with numpy, you can use numpy instead\n",
    "-  matplotlib: a python package for data visualization\n",
    "\n",
    "To install pytorch, please follow the __[instructions on their website](http://pytorch.org/)__. Please ensure you install version 0.4 or above.\n",
    "\n",
    "To install pandas, in your terminal, type `pip3 install pandas` for python 3.\n",
    "\n",
    "To install matplotlib, in your terminal, type `pip install matplotlib` for python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and setup training dataset\n",
    "\n",
    "The dataset is separated into two files from original dataset:\n",
    "-  iris_train.csv = dataset for training purpose, 80% from the original data\n",
    "-  iris_test.csv  = dataset for testing purpose, 20% from the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "data_train = pd.read_csv('dataset/iris_train.csv')\n",
    "\n",
    "# convert string target values to numeric values\n",
    "#       class 0: Iris-setosa\n",
    "#       class 1: Iris-versicolor\n",
    "#       class 2: Iris-virginica\n",
    "data_train.at[data_train['species'] == 'Iris-setosa', ['species']] = 0\n",
    "data_train.at[data_train['species'] == 'Iris-versicolor', ['species']] = 1\n",
    "data_train.at[data_train['species'] == 'Iris-virginica', ['species']] = 2\n",
    "\n",
    "# Also convert all string numeric values to int ['2' -> 2]\n",
    "data_train = data_train.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The dataset is the same as the Task1, hence the visualization code has been\n",
    "# commented out. To use it, simply uncomment it. \n",
    "# (Highlight code, and use Ctrl + /, to toggle multiline comment)\n",
    "\n",
    "# print(data_train)\n",
    "\n",
    "# # extract frequency of each species class\n",
    "# class_freq = data_train['species'].value_counts()\n",
    "# class_freq = list(class_freq.sort_index())\n",
    "\n",
    "# # x-axis labels and length\n",
    "# x_axis = list(range(0,3))\n",
    "\n",
    "# graph = plt.bar(x_axis, class_freq)\n",
    "# plt.xticks(x_axis)\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xlabel('Species')\n",
    "# plt.title('Training Data')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas dataframe to array\n",
    "# the first 4 columns are features\n",
    "# the last column is target\n",
    "data_train_array = data_train.values\n",
    "\n",
    "# split x (features) and y (targets)\n",
    "x_array = data_train_array[:, :4]\n",
    "y_array = data_train_array[:, 4]\n",
    "\n",
    "# create Tensors to hold inputs and outputs. Tensors are data structures\n",
    "# similar to numpy matrices. They can be operated on efficiently by a GPU\n",
    "# \n",
    "# Note: In torch versions before 0.4, Tensors had to be wrapped in a Variable\n",
    "# to be used by the NN.\n",
    "X = torch.tensor(x_array, dtype=torch.float)\n",
    "Y = torch.tensor(y_array, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define and train a neural network\n",
    "\n",
    "Here we build a neural network with one hidden layer.\n",
    "-  input layer: 4 neurons, representing the features of Iris\n",
    "-  hidden layer: 10 neurons, using Sigmoid as activation function\n",
    "-  output layer: 3 neurons, representing the classes of Iris\n",
    "    \n",
    "The network will be trained with Stochastic Gradient Descent (SGD) as an \n",
    "optimiser, that will hold the current state and will update the parameters\n",
    "based on the computed gradients.\n",
    "\n",
    "Its performance will be evaluated using cross-entropy.\n",
    "    \n",
    "We implement the same neural network as before using a custom Module subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of neurons for input layer, hidden layer and output layer\n",
    "# define learning rate and number of epoch on training\n",
    "# Note the more generalizable ways of defining number of input and output neurons\n",
    "\n",
    "input_neurons = x_array.shape[1]\n",
    "hidden_neurons = 10\n",
    "output_neurons = np.unique(y_array).size\n",
    "learning_rate = 0.01\n",
    "num_epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a customised neural network structure\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define linear hidden layer output\n",
    "        self.hidden = torch.nn.Linear(n_input, n_hidden)\n",
    "        # define linear output layer output\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            In the forward function we define the process of performing\n",
    "            forward pass, that is to accept a Variable of input\n",
    "            data, x, and return a Variable of output data, y_pred.\n",
    "        \"\"\"\n",
    "        # get hidden layer input\n",
    "        h_input = self.hidden(x)\n",
    "        # define activation function for hidden layer\n",
    "        h_output = F.sigmoid(h_input)\n",
    "        # get output layer output\n",
    "        y_pred = self.out(h_output)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a neural network using the customised structure\n",
    "net = TwoLayerNet(input_neurons, hidden_neurons, output_neurons)\n",
    "\n",
    "# define loss function\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimiser\n",
    "optimiser = torch.optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all losses for visualisation\n",
    "all_losses = []\n",
    "\n",
    "# train a neural network\n",
    "for epoch in range(num_epoch):\n",
    "    # Perform forward pass: compute predicted y by passing x to the model.\n",
    "    # Here we pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    # In this case, Y_pred contains three columns, where the index of the\n",
    "    # max column indicates the class of the instance\n",
    "    Y_pred = net(X)\n",
    "\n",
    "    # Compute loss\n",
    "    # Here we pass Tensors containing the predicted and true values of Y,\n",
    "    # and the loss function returns a Tensor containing the loss.\n",
    "    loss = loss_func(Y_pred, Y)\n",
    "    all_losses.append(loss.item())\n",
    "\n",
    "    # print progress\n",
    "    if epoch % 50 == 0:\n",
    "        # convert three-column predicted Y values to one column for comparison\n",
    "        _, predicted = torch.max(F.softmax(Y_pred,1), 1)\n",
    "\n",
    "        # calculate and print accuracy\n",
    "        total = predicted.size(0)\n",
    "        correct = predicted.data.numpy() == Y.data.numpy()\n",
    "\n",
    "        print('Epoch [%d/%d] Loss: %.4f  Accuracy: %.2f %%'\n",
    "              % (epoch + 1, num_epoch, loss.item(), 100 * sum(correct)/total))\n",
    "\n",
    "    # Clear the gradients before running the backward pass.\n",
    "    net.zero_grad()\n",
    "\n",
    "    # Perform backward pass: compute gradients of the loss with respect to\n",
    "    # all the learnable parameters of the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimiser makes an update to its\n",
    "    # parameters\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot historical loss from `all_losses` during network learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Results\n",
    "\n",
    "To see how well the network performs on different categories, we will\n",
    "create a confusion matrix, indicating for every iris flower (rows)\n",
    "which class the network guesses (columns). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = torch.zeros(output_neurons, output_neurons)\n",
    "\n",
    "Y_pred = net(X)\n",
    "_, predicted = torch.max(F.softmax(Y_pred,1), 1)\n",
    "\n",
    "for i in range(x_array.shape[0]):\n",
    "    actual_class = Y.data[i]\n",
    "    predicted_class = predicted.data[i]\n",
    "\n",
    "    confusion[actual_class][predicted_class] += 1\n",
    "\n",
    "print('')\n",
    "print('Confusion matrix for training:')\n",
    "print(confusion.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and setup testing dataset\n",
    "\n",
    "The dataset is separated into two files from original dataset:\n",
    "-  iris_train.csv = dataset for training purpose, 80% from the original data\n",
    "-  iris_test.csv  = dataset for testing purpose, 20% from the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing data\n",
    "data_test = pd.read_csv('dataset/iris_test.csv')\n",
    "\n",
    "# convert string target values to numeric values\n",
    "#       class 0: Iris-setosa\n",
    "#       class 1: Iris-versicolor\n",
    "#       class 2: Iris-virginica\n",
    "data_test.at[data_test['species'] == 'Iris-setosa', ['species']] = 0\n",
    "data_test.at[data_test['species'] == 'Iris-versicolor', ['species']] = 1\n",
    "data_test.at[data_test['species'] == 'Iris-virginica', ['species']] = 2\n",
    "\n",
    "# Also convert all string numeric values to int ['2' -> 2]\n",
    "data_test = data_test.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas dataframe to array\n",
    "# the first 4 columns are features\n",
    "# the last column is target\n",
    "data_test_array = data_test.values\n",
    "\n",
    "# split x (features) and y (targets)\n",
    "x_test_array = data_test_array[:, :4]\n",
    "y_test_array = data_test_array[:, 4]\n",
    "\n",
    "# create Tensors to hold inputs and outputs\n",
    "X_test = torch.tensor(x_test_array, dtype=torch.float)\n",
    "Y_test = torch.tensor(y_test_array, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the neural network\n",
    "\n",
    "Pass testing data to the built neural network and get its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the neural network using testing data\n",
    "# It is actually performing a forward pass computation of predicted y\n",
    "# by passing x to the model.\n",
    "# Here, Y_pred_test contains three columns, where the index of the\n",
    "# max column indicates the class of the instance\n",
    "Y_pred_test = net(X_test)\n",
    "\n",
    "# get prediction\n",
    "# convert three-column predicted Y values to one column for comparison\n",
    "_, predicted_test = torch.max(F.softmax(Y_pred_test,1), 1)\n",
    "\n",
    "# calculate accuracy\n",
    "total_test = predicted_test.size(0)\n",
    "correct_test = sum(predicted_test.data.numpy() == Y_test.data.numpy())\n",
    "\n",
    "print('Testing Accuracy: %.2f %%' % (100 * correct_test / total_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Results\n",
    "\n",
    "To see how well the network performs on different categories, we will\n",
    "create a confusion matrix, indicating for every iris flower (rows)\n",
    "which class the network guesses (columns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_test = torch.zeros(output_neurons, output_neurons)\n",
    "\n",
    "for i in range(x_test_array.shape[0]):\n",
    "    actual_class = Y_test.data[i]\n",
    "    predicted_class = predicted_test.data[i]\n",
    "\n",
    "    confusion_test[actual_class][predicted_class] += 1\n",
    "\n",
    "print('')\n",
    "print('Confusion matrix for testing:')\n",
    "print(confusion_test.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_**Q1. What is the classification accuracy?**_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Q2. Try running the network with different number of hidden neurons. What effect does it have on the accuracy?**_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Q3. What was the best accuracy you were able to achieve? What were the parameters of the neural network?**_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_**Q4. Run the neural network again using the same parameters as your best result. Did you receive the exact same result again? Why might it different?**_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Q5. If you have finished the other tasks, try extending the functionality of the neural network and playing around with the parameters, such as the number of hidden neurons and the number of hidden layers. You can try changing the activation functions to others to see what effect this has on the output and error. You can also look into the other types of neural networks and learning algorithms that PyTorch has available.**_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
