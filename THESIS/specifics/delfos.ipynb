{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [14:42:16] Enabling RDKit 2019.09.3 jupyter extensions\n",
      "/Users/u6676643/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#delfos copy for pka prediction\n",
    "import torch\n",
    "from torch import nn\n",
    "from mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit import Chem\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: define solvent X and solute Y using mol2vec (but don't add the substructures)\n",
    "#step 2: run RNN in both directions on each molecule, then concatenate forward;reverse to get H and G\n",
    "#step 3: feed H and G into attention layer, generate attention alignment matrix, create contexts P and Q\n",
    "#step 4: maxpool H;P and G;Q into 2D feature vectors\n",
    "#step 5: create flattened input u;v and feed into linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified sentence2vec function to return lists of word vectors\n",
    "def sentences2vecs(sentences, model, unseen=None):\n",
    "    \"\"\"Generate vectors for each word in a sentence sentence (list) in a list of sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list, array\n",
    "        List with sentences\n",
    "    model : word2vec.Word2Vec\n",
    "        Gensim word2vec model\n",
    "    unseen : None, str\n",
    "        Keyword for unseen words. If None, those words are skipped.\n",
    "        https://stats.stackexchange.com/questions/163005/how-to-set-the-dictionary-for-text-analysis-using-neural-networks/163032#163032\n",
    "    Returns\n",
    "    -------\n",
    "    list of arrays, each sentence -> array of word vectors\n",
    "    \"\"\"\n",
    "    keys = set(model.wv.key_to_index)\n",
    "    bigveclist = []\n",
    "    if unseen:\n",
    "        unseen_vec = model.wv.get_vector(unseen)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        veclist = []\n",
    "        if unseen:\n",
    "            veclist.append([model.wv.get_vector(y) if y in set(sentence) & keys\n",
    "                       else unseen_vec for y in sentence])\n",
    "        else:\n",
    "            veclist.append([model.wv.get_vector(y) for y in sentence \n",
    "                            if y in set(sentence) & keys])\n",
    "        vecarray = np.concatenate(veclist, axis=1)\n",
    "        vectensor = torch.Tensor(vecarray)\n",
    "        bigveclist.append(vectensor)\n",
    "    return bigveclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [14:57:12] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [14:57:12] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [14:57:12] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "#step 1: mol2vec embedding\n",
    "data = pd.read_csv('non_aqueous_pka_data.csv')\n",
    "\n",
    "#mol2vec model\n",
    "mol2vec_model = word2vec.Word2Vec.load('model_300dim.pkl')\n",
    "\n",
    "#create mol type\n",
    "data['sol_mol'] = data.apply(lambda x: Chem.MolFromSmiles(x['Solute SMILES']), axis=1)\n",
    "data['solv_mol'] = data.apply(lambda x: Chem.MolFromSmiles(x['Solvent SMILES']), axis=1)\n",
    "\n",
    "#remove invalid smiles\n",
    "#data.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "#data.dropna(subset = ['mol'], inplace=True)\n",
    "#print(data)\n",
    "\n",
    "#create sentences\n",
    "data['sol_sentence'] = data.apply(lambda x: mol2alt_sentence(x['sol_mol'],1), axis=1)\n",
    "data['solv_sentence'] = data.apply(lambda x: mol2alt_sentence(x['solv_mol'],1), axis=1)\n",
    "\n",
    "targets = torch.Tensor(data['pKa (avg)'])\n",
    "sol_data = sentences2vecs(data['sol_sentence'], mol2vec_model, unseen='UNK')\n",
    "solv_data = sentences2vecs(data['solv_sentence'], mol2vec_model, unseen='UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.nn.utils.rnn import pack_sequence\n",
    "#X = pack_sequence(X,enforce_sorted = False)\n",
    "##TODO BATCHING VARIABLE SEQUENCE LENGTHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(G,H):\n",
    "    alpha = torch.exp(H@torch.t(G))\n",
    "    norm = torch.sum(alpha, dim=1)\n",
    "    norm = torch.pow(norm, -1)\n",
    "    alpha = alpha * norm[:, None]\n",
    "    return alpha\n",
    "\n",
    "def att(G,H):\n",
    "    P = alpha(G,H)@G\n",
    "    inH = torch.cat((H,P),1)\n",
    "    return inH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "epochs = 10\n",
    "n_features = 300\n",
    "n_hidden = 100\n",
    "\n",
    "class maxpool(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(maxpool, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d((L,2), stride=2)\n",
    "    def forward(self, X):\n",
    "        return self.maxpool(X)\n",
    "\n",
    "class dnet(nn.Module):\n",
    "    def __init__(self, n_features, D, FF):\n",
    "        super(dnet, self).__init__()\n",
    "    \n",
    "        self.biLSTM_X = nn.LSTM(n_features, D, bidirectional=True)\n",
    "        self.biLSTM_Y = nn.LSTM(n_features, D, bidirectional=True)\n",
    "        \n",
    "        self.FF = nn.Linear(4*D, FF)\n",
    "        self.out = nn.Linear(FF, 1)\n",
    "    \n",
    "    def forward(self,X,Y):\n",
    "        N = X.data.shape[0]\n",
    "        M = Y.data.shape[0]\n",
    "        \n",
    "        #turn input list of vec into correct shape\n",
    "        X = X.view(X.data.shape[0],1,X.data.shape[1]) #N rows\n",
    "        Y = Y.view(Y.data.shape[0],1,Y.data.shape[1]) #M rows\n",
    "        \n",
    "        #biLSTM to get hidden states\n",
    "        H, hcX = self.biLSTM_X(X, None) #Nx1x2D matrix\n",
    "        G, hcY = self.biLSTM_Y(Y, None) #Mx1x2D matrix\n",
    "        \n",
    "        #contexts\n",
    "        inG = att(H[:,0,:],G[:,0,:]) #Nx4D\n",
    "        inH = att(G[:,0,:],H[:,0,:]) #Mx4D\n",
    "        \n",
    "        #maxpool concatenated tensors\n",
    "        maxpool_X = maxpool(N)\n",
    "        maxpool_Y = maxpool(M)\n",
    "        u = maxpool_X(inH.view(1,inH.data.shape[0],inH.data.shape[1]))  #1x1x2D\n",
    "        v = maxpool_Y(inG.view(1,inG.data.shape[0],inG.data.shape[1]))  #1x1x2D\n",
    "        \n",
    "        #feed forward neural network\n",
    "        NN = torch.cat((u,v),2)\n",
    "        NN = self.FF(NN)\n",
    "        NN = nn.functional.relu(NN)\n",
    "        output = self.out(NN)\n",
    "        return output\n",
    "\n",
    "dmodel = dnet(300,150,2000)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(dmodel.parameters(), lr=0.0002, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition, 3D maxpool\n",
    "epochs = 10\n",
    "n_features = 300\n",
    "n_hidden = 100\n",
    "\n",
    "class maxpool(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(maxpool, self).__init__()\n",
    "        self.maxpool = nn.MaxPool3d((L,1,2), stride=1)\n",
    "    def forward(self, X):\n",
    "        return self.maxpool(X)\n",
    "    \n",
    "def att(G,H):\n",
    "    Q = alpha(G,H)@G\n",
    "    inH = torch.stack([H,Q],2)\n",
    "    return inH\n",
    "\n",
    "class dnet(nn.Module):\n",
    "    def __init__(self, n_features, D, FF):\n",
    "        super(dnet, self).__init__()\n",
    "    \n",
    "        self.biLSTM_X = nn.LSTM(n_features, D, bidirectional=True)\n",
    "        self.biLSTM_Y = nn.LSTM(n_features, D, bidirectional=True)\n",
    "        \n",
    "        self.FF = nn.Linear(4*D, FF)\n",
    "        self.out = nn.Linear(FF, 1)\n",
    "    \n",
    "    def forward(self,X,Y):\n",
    "        N = X.data.shape[0]\n",
    "        M = Y.data.shape[0]\n",
    "        \n",
    "        #turn input list of vec into correct shape\n",
    "        X = X.view(X.data.shape[0],1,X.data.shape[1]) #N rows\n",
    "        Y = Y.view(Y.data.shape[0],1,Y.data.shape[1]) #M rows\n",
    "        \n",
    "        #biLSTM to get hidden states\n",
    "        H, hcX = self.biLSTM_X(X, None) #Nx1x2D matrix\n",
    "        G, hcY = self.biLSTM_Y(Y, None) #Mx1x2D matrix\n",
    "        \n",
    "        #contexts\n",
    "        inG = att(H[:,0,:],G[:,0,:]) #Nx4D\n",
    "        inH = att(G[:,0,:],H[:,0,:]) #Mx4D\n",
    "        \n",
    "        #maxpool concatenated tensors\n",
    "        maxpool_X = maxpool(N)\n",
    "        maxpool_Y = maxpool(M)\n",
    "        u = maxpool_X(inH.view(1,inH.data.shape[0],inH.data.shape[1],inH.data.shape[2]))  #1x1x2D\n",
    "        v = maxpool_Y(inG.view(1,inG.data.shape[0],inG.data.shape[1],inG.data.shape[2]))  #1x1x2D\n",
    "        \n",
    "        #feed forward neural network\n",
    "        NN = torch.cat((u,v),2).view(1,1,600)\n",
    "        NN = self.FF(NN)\n",
    "        NN = nn.functional.relu(NN)\n",
    "        output = self.out(NN)\n",
    "        return output\n",
    "\n",
    "dmodel = dnet(300,150,2000)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(dmodel.parameters(), lr=0.0002, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: 34.07905578613281  target: 35.0\n",
      "output: 37.32956314086914  target: 35.70000076293945\n",
      "output: 32.2772331237793  target: 31.700000762939453\n",
      "output: 9.855939865112305  target: 8.800000190734863\n",
      "output: 45.4583625793457  target: 43.599998474121094\n",
      "output: 14.228582382202148  target: 13.5\n",
      "output: 11.161382675170898  target: 7.960000038146973\n",
      "output: 10.684391021728516  target: 10.100000381469727\n",
      "output: 19.549884796142578  target: 16.610000610351562\n",
      "output: 9.865285873413086  target: 9.210000038146973\n",
      "output: 9.979820251464844  target: 10.5\n",
      "output: 18.52896499633789  target: 16.459999084472656\n",
      "output: 10.483538627624512  target: 6.170000076293945\n",
      "output: 9.402362823486328  target: 7.46999979019165\n",
      "output: 40.28449630737305  target: 40.400001525878906\n",
      "output: 8.161394119262695  target: 8.774999618530273\n",
      "output: 7.7627081871032715  target: 6.190000057220459\n",
      "output: 7.659736156463623  target: 6.704999923706055\n",
      "output: 7.580693244934082  target: 7.860000133514404\n",
      "output: 6.729977130889893  target: 6.039999961853027\n",
      "output: 37.223243713378906  target: 39.900001525878906\n",
      "output: 2.6962499618530273  target: 3.3299999237060547\n",
      "output: 32.434574127197266  target: 35.900001525878906\n",
      "output: 36.011146545410156  target: 37.099998474121094\n",
      "output: 3.2903671264648438  target: 4.215000152587891\n",
      "output: 6.399797439575195  target: 6.599999904632568\n",
      "output: 41.3443603515625  target: 40.70000076293945\n",
      "output: 6.981105327606201  target: 7.829999923706055\n",
      "output: 10.136846542358398  target: 9.489999771118164\n",
      "output: 4.281905174255371  target: 4.880000114440918\n",
      "output: 21.265785217285156  target: 20.1299991607666\n",
      "output: 24.975406646728516  target: 24.959999084472656\n",
      "output: -8.876340866088867  target: -8.800000190734863\n",
      "output: -7.435304164886475  target: -6.800000190734863\n",
      "output: 40.4172477722168  target: 40.599998474121094\n",
      "output: 4.645099639892578  target: 5.471428394317627\n",
      "output: 7.024118423461914  target: 7.300000190734863\n",
      "output: 35.011871337890625  target: 33.0\n",
      "output: 4.674060344696045  target: 4.375\n",
      "output: 36.56243133544922  target: 36.29999923706055\n",
      "output: 35.558650970458984  target: 33.20000076293945\n",
      "output: 34.86008071899414  target: 29.700000762939453\n",
      "output: 33.6876220703125  target: 32.900001525878906\n",
      "output: 37.12019348144531  target: 38.20000076293945\n",
      "output: 6.312827110290527  target: 5.0\n",
      "output: -4.384922504425049  target: -5.099999904632568\n",
      "output: 34.73516845703125  target: 38.599998474121094\n",
      "output: 40.342750549316406  target: 42.20000076293945\n",
      "output: 8.760958671569824  target: 8.859999656677246\n",
      "output: 40.832801818847656  target: 44.20000076293945\n",
      "output: 0.13567256927490234  target: 1.7000000476837158\n",
      "output: 10.471744537353516  target: 10.1899995803833\n",
      "output: 11.02096939086914  target: 13.010000228881836\n",
      "output: 11.908258438110352  target: 12.229999542236328\n",
      "output: 43.211883544921875  target: 46.29999923706055\n",
      "output: 34.280670166015625  target: 33.5\n",
      "output: 19.293914794921875  target: 16.125\n",
      "output: 21.35136604309082  target: 20.1299991607666\n",
      "output: 7.3175048828125  target: 7.5\n",
      "output: 21.435564041137695  target: 20.360000610351562\n",
      "output: 54.39955520629883  target: 50.5\n",
      "output: 2.445389747619629  target: 3.299999952316284\n",
      "output: 15.006359100341797  target: 16.39666748046875\n",
      "output: 50.27165603637695  target: 47.29999923706055\n",
      "output: 1.5855884552001953  target: 2.4000000953674316\n",
      "output: 12.839006423950195  target: 13.466666221618652\n",
      "output: 19.612342834472656  target: 20.076000213623047\n",
      "output: 54.82234191894531  target: 54.400001525878906\n",
      "output: 14.239991188049316  target: 16.014999389648438\n",
      "output: 53.86986541748047  target: 53.266666412353516\n",
      "output: 19.2767276763916  target: 19.31999969482422\n",
      "output: 0.37742137908935547  target: -0.6800000071525574\n",
      "output: 39.96868133544922  target: 37.5\n",
      "output: 15.60205078125  target: 17.200000762939453\n",
      "output: 26.373876571655273  target: 28.780000686645508\n",
      "output: 5.445895195007324  target: 5.639999866485596\n",
      "output: 32.264892578125  target: 34.29999923706055\n",
      "output: 15.843897819519043  target: 19.200000762939453\n",
      "output: 13.531673431396484  target: 14.5\n",
      "output: 14.032304763793945  target: 13.699999809265137\n",
      "output: 10.12584114074707  target: 10.75\n",
      "output: 11.91053295135498  target: 12.649999618530273\n",
      "output: 13.829780578613281  target: 13.800000190734863\n",
      "output: 12.861111640930176  target: 13.199999809265137\n",
      "output: 17.186086654663086  target: 15.300000190734863\n",
      "output: 18.642377853393555  target: 18.799999237060547\n",
      "output: 19.499282836914062  target: 19.299999237060547\n",
      "output: 17.77385139465332  target: 17.100000381469727\n",
      "output: 14.968884468078613  target: 15.100000381469727\n",
      "output: 29.382667541503906  target: 32.599998474121094\n",
      "output: 20.858549118041992  target: 20.524999618530273\n",
      "output: 7.218255043029785  target: 5.329999923706055\n",
      "output: 10.123034477233887  target: 10.699999809265137\n",
      "output: 31.825531005859375  target: 32.70000076293945\n",
      "output: 33.880313873291016  target: 33.5\n",
      "output: 33.55718231201172  target: 32.599998474121094\n",
      "output: 15.988595962524414  target: 14.800000190734863\n",
      "output: 10.69206428527832  target: 10.470000267028809\n",
      "output: 12.088274955749512  target: 12.300000190734863\n",
      "output: 20.889726638793945  target: 19.760000228881836\n",
      "output: 33.762489318847656  target: 32.599998474121094\n",
      "output: 20.145751953125  target: 18.0\n",
      "output: 19.22698211669922  target: 17.600000381469727\n",
      "output: 20.863222122192383  target: 19.200000762939453\n",
      "output: 10.960800170898438  target: 11.699999809265137\n",
      "output: 19.41343116760254  target: 19.139999389648438\n",
      "output: 14.160442352294922  target: 14.199999809265137\n",
      "output: 9.310235023498535  target: 10.850000381469727\n",
      "output: 20.184173583984375  target: 20.299999237060547\n",
      "output: 20.746681213378906  target: 20.899999618530273\n",
      "output: 12.998794555664062  target: 15.199999809265137\n",
      "output: 9.126091003417969  target: 10.649999618530273\n",
      "output: 10.714975357055664  target: 12.600000381469727\n",
      "output: 18.92588996887207  target: 20.1200008392334\n",
      "output: 12.624702453613281  target: 13.600000381469727\n",
      "output: 8.912748336791992  target: 9.979999542236328\n",
      "output: 10.453390121459961  target: 11.100000381469727\n",
      "output: 18.1209716796875  target: 18.459999084472656\n",
      "output: 18.34838104248047  target: 21.0\n",
      "output: 33.95448303222656  target: 33.0\n",
      "output: 4.805956840515137  target: 7.829999923706055\n",
      "output: -11.910149574279785  target: -14.699999809265137\n",
      "output: 33.66774368286133  target: 33.70000076293945\n",
      "output: -9.495158195495605  target: -14.300000190734863\n",
      "output: 1.2516775131225586  target: 2.5999999046325684\n",
      "output: 30.963180541992188  target: 31.399999618530273\n",
      "output: 12.440362930297852  target: 12.699999809265137\n",
      "output: 10.119424819946289  target: 9.760000228881836\n",
      "output: 10.373620986938477  target: 8.399999618530273\n",
      "output: 17.435089111328125  target: 17.610000610351562\n",
      "output: 21.89443016052246  target: 23.325000762939453\n",
      "output: 17.01500129699707  target: 15.5\n",
      "output: 13.430328369140625  target: 13.300000190734863\n",
      "output: 14.10529899597168  target: 13.199999809265137\n",
      "output: 5.283811569213867  target: 4.900000095367432\n",
      "output: 4.0697407722473145  target: 5.059999942779541\n",
      "output: 3.203355312347412  target: 2.700000047683716\n",
      "output: 10.194601058959961  target: 11.449999809265137\n",
      "output: 11.335163116455078  target: 11.100000381469727\n",
      "output: 9.296987533569336  target: 12.0\n",
      "output: 9.03454875946045  target: 7.46999979019165\n",
      "output: 17.044540405273438  target: 18.6299991607666\n",
      "output: 18.25847625732422  target: 19.959999084472656\n",
      "output: 10.35434341430664  target: 10.800000190734863\n",
      "output: 13.336556434631348  target: 13.100000381469727\n",
      "output: 10.013509750366211  target: 10.0\n",
      "output: 10.798463821411133  target: 10.600000381469727\n",
      "output: 13.625059127807617  target: 13.0\n",
      "output: 10.144661903381348  target: 9.399999618530273\n",
      "output: 19.4683780670166  target: 19.280000686645508\n",
      "output: 12.413536071777344  target: 12.800000190734863\n",
      "output: 9.855674743652344  target: 9.149999618530273\n",
      "output: 8.709075927734375  target: 8.800000190734863\n",
      "output: 18.300539016723633  target: 18.690000534057617\n",
      "output: 6.807233810424805  target: 6.519999980926514\n",
      "output: 11.54006576538086  target: 12.899999618530273\n",
      "output: 8.300697326660156  target: 10.079999923706055\n",
      "output: 8.71082878112793  target: 8.399999618530273\n",
      "output: 17.663436889648438  target: 18.239999771118164\n",
      "output: 11.696968078613281  target: 12.399999618530273\n",
      "output: 8.223871231079102  target: 8.539999961853027\n",
      "output: 8.449859619140625  target: 7.699999809265137\n",
      "output: 17.71951675415039  target: 17.3799991607666\n",
      "output: 18.237808227539062  target: 19.030000686645508\n",
      "output: 12.780313491821289  target: 13.300000190734863\n",
      "output: 10.91813850402832  target: 10.640000343322754\n",
      "output: 10.363188743591309  target: 10.300000190734863\n",
      "output: 6.055303573608398  target: 4.800000190734863\n",
      "output: 3.96608304977417  target: 4.849999904632568\n",
      "output: 2.9582135677337646  target: 2.940000057220459\n",
      "output: 11.497222900390625  target: 10.970000267028809\n",
      "output: 17.961029052734375  target: 18.3700008392334\n",
      "output: 9.593974113464355  target: 10.619999885559082\n",
      "output: 12.74472427368164  target: 13.5\n",
      "output: 9.47701358795166  target: 11.0\n",
      "output: 18.840129852294922  target: 17.81999969482422\n",
      "output: 5.92263126373291  target: 4.820000171661377\n",
      "output: 26.342493057250977  target: 24.579999923706055\n",
      "output: 14.593195915222168  target: 12.5\n",
      "output: 2.5357465744018555  target: 3.0999999046325684\n",
      "output: 14.750961303710938  target: 10.460000038146973\n",
      "output: 0.2239670753479004  target: 1.3200000524520874\n",
      "output: 10.716056823730469  target: 10.109999656677246\n",
      "output: 5.432900428771973  target: 2.3499999046325684\n",
      "output: 18.898406982421875  target: 18.010000228881836\n",
      "output: 9.45849323272705  target: 7.96999979019165\n",
      "output: 4.258334159851074  target: 4.739999771118164\n",
      "output: 5.774145126342773  target: 7.179999828338623\n",
      "output: 20.976070404052734  target: 22.149999618530273\n",
      "output: 34.297584533691406  target: 35.900001525878906\n",
      "output: 22.581613540649414  target: 22.0\n",
      "output: 10.879268646240234  target: 11.600000381469727\n",
      "output: 23.015277862548828  target: 23.979999542236328\n",
      "output: 19.952381134033203  target: 20.104999542236328\n",
      "output: 6.102166175842285  target: 5.550000190734863\n",
      "output: 8.275054931640625  target: 8.899999618530273\n",
      "output: 15.452277183532715  target: 16.613332748413086\n",
      "output: 19.297725677490234  target: 18.104999542236328\n",
      "output: 19.489912033081055  target: 22.270000457763672\n",
      "output: 7.840510368347168  target: 8.300000190734863\n",
      "output: 7.190243244171143  target: 5.110000133514404\n",
      "output: 8.997196197509766  target: 11.0\n",
      "output: 14.76564884185791  target: 15.395000457763672\n",
      "output: 9.825580596923828  target: 10.720000267028809\n",
      "output: -0.13022661209106445  target: 0.800000011920929\n",
      "output: 32.071590423583984  target: 31.899999618530273\n",
      "output: -1.3539748191833496  target: 0.0\n",
      "output: 4.5261945724487305  target: 5.300000190734863\n",
      "output: 52.25469207763672  target: 53.79999923706055\n",
      "output: 20.600173950195312  target: 18.875999450683594\n",
      "output: 10.106760025024414  target: 7.949999809265137\n",
      "output: 4.6674604415893555  target: 2.5\n",
      "output: 3.584794759750366  target: 1.1699999570846558\n",
      "output: 1.486185073852539  target: 0.0\n",
      "output: 8.562698364257812  target: 6.340000152587891\n",
      "output: 0.7384986877441406  target: -0.1599999964237213\n",
      "output: 48.86100387573242  target: 44.099998474121094\n",
      "output: 9.713626861572266  target: 12.979999542236328\n",
      "output: 35.700355529785156  target: 0.0\n",
      "output: 18.31220817565918  target: 21.106666564941406\n",
      "output: 42.756103515625  target: 55.70000076293945\n",
      "output: 6.727431297302246  target: 5.670000076293945\n",
      "output: 17.856948852539062  target: 20.110000610351562\n",
      "output: 30.71145248413086  target: 29.899999618530273\n",
      "output: 24.35097312927246  target: 23.8700008392334\n",
      "output: 11.606409072875977  target: 12.199999809265137\n",
      "output: 13.122798919677734  target: 10.539999961853027\n",
      "output: 4.124553203582764  target: 1.2799999713897705\n",
      "output: -0.5072841644287109  target: 2.630000114440918\n",
      "output: 9.692469596862793  target: 11.0\n",
      "output: 1.2723283767700195  target: 1.100000023841858\n",
      "output: 11.55705451965332  target: 11.84000015258789\n",
      "output: -2.3837780952453613  target: 0.3499999940395355\n",
      "output: 32.86674880981445  target: 44.86666488647461\n",
      "output: 0.12405061721801758  target: -1.3300000429153442\n",
      "output: 12.844500541687012  target: 15.300000190734863\n",
      "output: 13.87972640991211  target: 16.170000076293945\n",
      "output: 0.9907245635986328  target: 2.009999990463257\n",
      "output: 13.864286422729492  target: 14.229999542236328\n",
      "output: 26.116918563842773  target: 33.599998474121094\n",
      "output: 7.081405162811279  target: 6.099999904632568\n",
      "output: 8.991071701049805  target: 8.699999809265137\n",
      "output: 19.08980369567871  target: 20.344999313354492\n",
      "output: 20.688701629638672  target: 22.5\n",
      "output: 20.091690063476562  target: 22.5\n",
      "output: 6.808530807495117  target: 4.46999979019165\n",
      "output: 35.100372314453125  target: 38.400001525878906\n",
      "output: 34.96211242675781  target: 39.0\n",
      "output: 7.463681221008301  target: 5.242499828338623\n",
      "output: 19.66591453552246  target: 17.0\n",
      "output: 20.748523712158203  target: 20.5\n",
      "output: 22.52743911743164  target: 22.84000015258789\n",
      "output: 8.902189254760742  target: 8.029999732971191\n",
      "output: 11.788907051086426  target: 13.199999809265137\n",
      "output: 24.31508445739746  target: 23.790000915527344\n",
      "output: 21.396228790283203  target: 18.700000762939453\n",
      "output: 25.000137329101562  target: 23.299999237060547\n",
      "output: 20.704954147338867  target: 18.799999237060547\n",
      "output: 7.316976070404053  target: 8.720000267028809\n",
      "output: 3.155122995376587  target: 2.4000000953674316\n",
      "output: 1.1921048164367676  target: 1.2999999523162842\n",
      "output: -0.7163591384887695  target: 0.550000011920929\n",
      "output: 4.597404956817627  target: 3.380000114440918\n",
      "output: -0.6997261047363281  target: 0.5\n",
      "output: 14.306365013122559  target: 14.770000457763672\n",
      "output: 15.157384872436523  target: 15.239999771118164\n",
      "output: 14.598563194274902  target: 15.800000190734863\n",
      "output: 20.200469970703125  target: 16.739999771118164\n",
      "output: 9.205322265625  target: 6.179999828338623\n",
      "output: 4.1478753089904785  target: 3.700000047683716\n",
      "output: 5.337395668029785  target: 4.900000095367432\n",
      "output: 15.809609413146973  target: 17.600000381469727\n",
      "output: 13.583820343017578  target: 12.600000381469727\n",
      "output: 15.916393280029297  target: 16.100000381469727\n",
      "output: 6.047904014587402  target: 6.329999923706055\n",
      "output: 20.34769058227539  target: 16.940000534057617\n",
      "output: 2.794389247894287  target: 4.099999904632568\n",
      "output: 4.043709754943848  target: 5.25\n",
      "output: 16.386817932128906  target: 16.6560001373291\n",
      "output: 39.293487548828125  target: 36.0\n",
      "output: 3.0263888835906982  target: 3.9800000190734863\n",
      "output: 3.5940983295440674  target: 3.6600000858306885\n",
      "output: 5.000673294067383  target: 7.670000076293945\n",
      "output: 17.510902404785156  target: 18.5\n",
      "output: 18.72613525390625  target: 19.0\n",
      "output: 3.9747142791748047  target: 3.799999952316284\n",
      "output: 4.262423992156982  target: 5.635000228881836\n",
      "output: 17.31451416015625  target: 18.399999618530273\n",
      "output: 6.401973724365234  target: 6.199999809265137\n",
      "output: 6.580151557922363  target: 8.390000343322754\n",
      "output: 17.77783203125  target: 17.899999618530273\n",
      "output: 18.90520477294922  target: 19.299999237060547\n",
      "output: 18.873872756958008  target: 19.899999618530273\n",
      "output: 23.2249755859375  target: 24.0\n",
      "output: 23.710458755493164  target: 23.290000915527344\n",
      "output: 10.292728424072266  target: 9.550000190734863\n",
      "output: 18.077367782592773  target: 21.899999618530273\n",
      "output: 18.43749237060547  target: 19.200000762939453\n",
      "output: 19.869264602661133  target: 19.5\n",
      "output: 19.149147033691406  target: 19.299999237060547\n",
      "output: 37.63223648071289  target: 49.0\n",
      "output: 15.611427307128906  target: 14.71500015258789\n",
      "output: 25.829126358032227  target: 25.0\n",
      "output: 27.893024444580078  target: 23.799999237060547\n",
      "output: 22.308605194091797  target: 23.799999237060547\n",
      "output: 14.74424934387207  target: 15.699999809265137\n",
      "output: 11.635032653808594  target: 9.079999923706055\n",
      "output: 14.513845443725586  target: 15.100000381469727\n",
      "output: 26.57884407043457  target: 26.18000030517578\n",
      "output: 10.59184455871582  target: 9.34000015258789\n",
      "output: 13.953582763671875  target: 14.899999618530273\n",
      "output: 25.381797790527344  target: 23.790000915527344\n",
      "output: 14.21621036529541  target: 15.199999809265137\n",
      "output: 25.84062957763672  target: 24.364999771118164\n",
      "output: 14.305392265319824  target: 16.010000228881836\n",
      "output: 10.104450225830078  target: 9.6899995803833\n",
      "output: 23.709491729736328  target: 23.229999542236328\n",
      "output: 9.955387115478516  target: 9.170000076293945\n",
      "output: 45.26910400390625  target: 42.400001525878906\n",
      "output: 8.99008846282959  target: 8.15666675567627\n",
      "output: 7.808326721191406  target: 6.614999771118164\n",
      "output: 23.06505584716797  target: 20.700000762939453\n",
      "output: 24.434823989868164  target: 23.739999771118164\n",
      "output: 14.206106185913086  target: 15.489999771118164\n",
      "output: 39.83857727050781  target: 37.099998474121094\n",
      "output: 4.378663063049316  target: 4.070000171661377\n",
      "output: 41.06616973876953  target: 41.099998474121094\n",
      "output: 6.979745864868164  target: 8.176666259765625\n",
      "output: 8.189164161682129  target: 9.170000076293945\n",
      "output: 40.308815002441406  target: 42.400001525878906\n",
      "output: 4.670673370361328  target: 6.400000095367432\n",
      "output: 6.306593418121338  target: 6.21999979019165\n",
      "output: 10.199711799621582  target: 9.430000305175781\n",
      "output: 5.047606468200684  target: 4.6566667556762695\n",
      "output: 36.07594680786133  target: 31.700000762939453\n",
      "output: 6.13425350189209  target: 5.226666450500488\n",
      "output: 34.501922607421875  target: 32.70000076293945\n",
      "output: 10.15799331665039  target: 9.489999771118164\n",
      "output: 11.874914169311523  target: 10.359999656677246\n",
      "output: 10.304428100585938  target: 10.649999618530273\n",
      "output: 17.878395080566406  target: 18.700000762939453\n",
      "output: 20.27349090576172  target: 20.299999237060547\n",
      "output: 18.801664352416992  target: 19.700000762939453\n",
      "output: 20.21958351135254  target: 20.799999237060547\n",
      "output: 2.6162877082824707  target: 2.299999952316284\n",
      "output: 11.976892471313477  target: 11.609999656677246\n",
      "output: 21.892744064331055  target: 19.0\n",
      "output: 8.687822341918945  target: 8.029999732971191\n",
      "output: 23.380130767822266  target: 25.520000457763672\n",
      "output: 9.234819412231445  target: 8.680000305175781\n",
      "output: 13.976832389831543  target: 14.600000381469727\n",
      "output: 21.551483154296875  target: 25.239999771118164\n",
      "output: 9.163373947143555  target: 9.640000343322754\n",
      "output: 18.572914123535156  target: 20.700000762939453\n",
      "output: 21.337764739990234  target: 22.700000762939453\n",
      "output: 24.089141845703125  target: 25.399999618530273\n",
      "output: 12.010709762573242  target: 11.949999809265137\n",
      "output: 22.974103927612305  target: 24.440000534057617\n",
      "output: 23.36286163330078  target: 27.0\n",
      "output: 16.7911319732666  target: 15.899999618530273\n",
      "output: 25.249927520751953  target: 25.174999237060547\n",
      "output: 17.930631637573242  target: 16.670000076293945\n",
      "output: 12.047346115112305  target: 10.020000457763672\n",
      "output: 24.650047302246094  target: 27.950000762939453\n",
      "output: 23.221759796142578  target: 26.18000030517578\n",
      "output: 27.120481491088867  target: 25.530000686645508\n",
      "output: 27.36284828186035  target: 25.440000534057617\n",
      "output: 25.345489501953125  target: 24.040000915527344\n",
      "output: 10.596817016601562  target: 9.789999961853027\n",
      "output: 15.98397159576416  target: 15.600000381469727\n",
      "output: 25.29832649230957  target: 25.850000381469727\n",
      "output: 16.254541397094727  target: 16.579999923706055\n",
      "output: 5.389286041259766  target: 5.019999980926514\n",
      "output: 10.051926612854004  target: 10.199999809265137\n",
      "output: 2.514756917953491  target: 3.4600000381469727\n",
      "output: 8.495224952697754  target: 7.142499923706055\n",
      "output: 14.286542892456055  target: 14.399999618530273\n",
      "output: 28.530853271484375  target: 27.790000915527344\n",
      "output: 5.23982572555542  target: 5.5\n",
      "output: 4.341182708740234  target: 5.25\n",
      "output: 4.523453712463379  target: 3.450000047683716\n",
      "output: 12.013286590576172  target: 12.529999732971191\n",
      "output: 10.950309753417969  target: 12.265000343322754\n",
      "output: 3.698514461517334  target: 4.385000228881836\n",
      "output: 7.848461151123047  target: 6.789999961853027\n",
      "output: 4.016623497009277  target: 5.5\n",
      "output: 3.850764751434326  target: 5.230000019073486\n",
      "output: 3.4298086166381836  target: 3.4000000953674316\n",
      "output: 11.46000862121582  target: 12.529999732971191\n",
      "output: 11.938318252563477  target: 12.199999809265137\n",
      "output: 6.948370933532715  target: 7.21999979019165\n",
      "output: 11.660676956176758  target: 11.0\n",
      "output: 22.757673263549805  target: 22.850000381469727\n",
      "output: 6.238277435302734  target: 4.800000190734863\n",
      "output: 8.135608673095703  target: 7.860000133514404\n",
      "output: 21.021202087402344  target: 18.200000762939453\n",
      "output: 20.572063446044922  target: 14.300000190734863\n",
      "output: 22.04488182067871  target: 19.0\n",
      "output: 19.806358337402344  target: 16.700000762939453\n",
      "output: 25.6660213470459  target: 24.889999389648438\n",
      "output: 10.176555633544922  target: 8.420000076293945\n",
      "output: 13.850954055786133  target: 14.399999618530273\n",
      "output: 16.417713165283203  target: 15.699999809265137\n",
      "output: 21.822444915771484  target: 23.920000076293945\n",
      "output: 18.999738693237305  target: 19.34000015258789\n",
      "output: 3.3861477375030518  target: 1.2799999713897705\n",
      "output: -3.199191093444824  target: -3.9100000858306885\n",
      "output: 3.2085835933685303  target: 0.6000000238418579\n",
      "output: 0.16788578033447266  target: 0.7900000214576721\n",
      "output: -0.06607580184936523  target: -1.100000023841858\n",
      "output: 5.999209403991699  target: 5.980000019073486\n",
      "output: 4.235566139221191  target: 5.199999809265137\n",
      "output: 2.745793581008911  target: 3.5950000286102295\n",
      "output: 8.470436096191406  target: 10.626667022705078\n",
      "output: 3.9497718811035156  target: 4.610000133514404\n",
      "output: 27.131315231323242  target: 35.5\n",
      "output: 27.641708374023438  target: 33.29999923706055\n",
      "output: 3.3823471069335938  target: 5.699999809265137\n",
      "output: 9.821817398071289  target: 11.0600004196167\n",
      "output: 9.647132873535156  target: 12.300000190734863\n",
      "output: 18.63060760498047  target: 25.110000610351562\n",
      "output: 5.599460124969482  target: 4.224999904632568\n",
      "output: 10.047215461730957  target: 11.050000190734863\n",
      "output: 21.427337646484375  target: 21.506000518798828\n",
      "output: 10.681705474853516  target: 11.800000190734863\n",
      "output: 13.48106575012207  target: 12.800000190734863\n",
      "output: 25.878925323486328  target: 26.139999389648438\n",
      "output: 6.499995708465576  target: 7.802000045776367\n",
      "output: -1.9079532623291016  target: 2.0\n",
      "output: 5.959212303161621  target: 6.5\n",
      "output: 4.907419681549072  target: 7.440000057220459\n",
      "output: 5.371604919433594  target: 4.099999904632568\n",
      "output: 12.492537498474121  target: 12.899999618530273\n",
      "output: 8.555610656738281  target: 8.520000457763672\n",
      "output: 12.496761322021484  target: 12.100000381469727\n",
      "output: 11.38818073272705  target: 9.34000015258789\n",
      "output: 11.619945526123047  target: 10.15999984741211\n",
      "output: 17.72124481201172  target: 16.920000076293945\n",
      "output: 15.318012237548828  target: 12.699999809265137\n",
      "output: 20.21922492980957  target: 14.550000190734863\n",
      "output: 30.119783401489258  target: 27.190000534057617\n",
      "output: 18.837663650512695  target: 17.899999618530273\n",
      "output: 9.90886116027832  target: 10.020000457763672\n",
      "output: 13.62900161743164  target: 13.899999618530273\n",
      "output: 9.368121147155762  target: 8.600000381469727\n",
      "output: 10.98124885559082  target: 8.300000190734863\n",
      "output: 19.9389705657959  target: 24.857500076293945\n",
      "output: 25.696199417114258  target: 29.170000076293945\n",
      "output: 18.041908264160156  target: 18.399999618530273\n",
      "output: 10.986611366271973  target: 9.989999771118164\n",
      "output: 15.411808013916016  target: 18.0\n",
      "output: 38.49593734741211  target: 33.599998474121094\n",
      "output: 13.432862281799316  target: 16.100000381469727\n",
      "output: 24.28447723388672  target: 25.6200008392334\n",
      "output: 16.856060028076172  target: 17.059999465942383\n",
      "output: 9.432952880859375  target: 10.100000381469727\n",
      "output: 6.558610439300537  target: 5.539999961853027\n",
      "output: 8.922063827514648  target: 11.34333324432373\n",
      "output: 3.0672860145568848  target: 3.6600000858306885\n",
      "output: 7.077398300170898  target: 8.199999809265137\n",
      "output: 9.449865341186523  target: 7.400000095367432\n",
      "output: 2.525704860687256  target: 0.5600000023841858\n",
      "output: 23.47292709350586  target: 22.770000457763672\n",
      "output: 10.07461929321289  target: 10.119999885559082\n",
      "output: 26.17567253112793  target: 26.010000228881836\n",
      "output: 20.96535873413086  target: 21.229999542236328\n",
      "output: 12.908252716064453  target: 14.569999694824219\n",
      "output: 2.744691848754883  target: 1.7999999523162842\n",
      "output: 4.150886058807373  target: 4.0\n",
      "output: 56.74803924560547  target: 55.900001525878906\n",
      "output: 9.915229797363281  target: 8.300000190734863\n",
      "output: 22.589080810546875  target: 21.263334274291992\n",
      "output: 7.631587982177734  target: 10.699999809265137\n",
      "output: 26.285118103027344  target: 28.125\n",
      "output: 10.548194885253906  target: 9.1899995803833\n",
      "output: 3.4388856887817383  target: 1.9500000476837158\n",
      "output: 7.168796062469482  target: 7.610000133514404\n",
      "output: 1.2472844123840332  target: 1.25\n",
      "output: 7.203763961791992  target: 5.085000038146973\n",
      "output: 10.920370101928711  target: 13.3100004196167\n",
      "output: 22.17929458618164  target: 23.639999389648438\n",
      "output: 3.1113815307617188  target: 2.2899999618530273\n",
      "output: 10.161548614501953  target: 10.5\n",
      "output: 13.67265796661377  target: 12.664999961853027\n",
      "output: 6.058797836303711  target: 5.619999885559082\n",
      "output: 4.312049865722656  target: 3.6600000858306885\n",
      "output: 11.48131275177002  target: 11.279999732971191\n",
      "output: 4.575470924377441  target: 4.929999828338623\n",
      "output: 12.63053035736084  target: 11.96500015258789\n",
      "output: 9.303857803344727  target: 9.770000457763672\n",
      "output: 8.434160232543945  target: 7.894000053405762\n",
      "output: 4.859079360961914  target: 4.25\n",
      "output: 12.686223030090332  target: 10.84000015258789\n",
      "output: 12.104469299316406  target: 11.960000038146973\n",
      "output: 3.758497476577759  target: 5.150000095367432\n",
      "output: 20.296438217163086  target: 20.950000762939453\n",
      "output: 30.59016990661621  target: 32.56999969482422\n",
      "output: 23.678529739379883  target: 21.649999618530273\n",
      "output: 16.871213912963867  target: 16.969999313354492\n",
      "output: 25.943567276000977  target: 26.15999984741211\n",
      "output: 12.259121894836426  target: 12.420000076293945\n",
      "output: 4.701265335083008  target: 4.639999866485596\n",
      "output: 4.660229682922363  target: 4.360000133514404\n",
      "output: 12.783293724060059  target: 13.529999732971191\n",
      "output: 5.410533428192139  target: 5.559999942779541\n",
      "output: 23.094240188598633  target: 22.399999618530273\n",
      "output: 23.520957946777344  target: 28.399999618530273\n",
      "output: 22.522123336791992  target: 25.700000762939453\n",
      "output: 9.838069915771484  target: 12.6850004196167\n",
      "output: 2.845022678375244  target: 5.460000038146973\n",
      "output: 10.390203475952148  target: 11.550000190734863\n",
      "output: 3.532641649246216  target: 3.4700000286102295\n",
      "output: 11.352437973022461  target: 10.5\n",
      "output: 24.575275421142578  target: 23.90999984741211\n",
      "output: 1.6334047317504883  target: 1.7999999523162842\n",
      "output: 10.619075775146484  target: 10.0649995803833\n",
      "output: 4.352553844451904  target: 3.700000047683716\n",
      "output: 2.5744268894195557  target: 2.3299999237060547\n",
      "output: 6.702357292175293  target: 6.885000228881836\n",
      "output: 0.43419313430786133  target: 0.41999998688697815\n",
      "output: 31.277759552001953  target: 29.989999771118164\n",
      "output: 7.180608749389648  target: 5.400000095367432\n",
      "output: 5.777759552001953  target: 4.610000133514404\n",
      "output: 12.735986709594727  target: 10.989999771118164\n",
      "output: 23.448368072509766  target: 21.799999237060547\n",
      "output: 26.743602752685547  target: 22.799999237060547\n",
      "output: 27.417953491210938  target: 20.299999237060547\n",
      "output: 29.508224487304688  target: 27.889999389648438\n",
      "output: 14.3030366897583  target: 14.75\n",
      "output: 17.3281307220459  target: 15.0\n",
      "output: 26.98771858215332  target: 28.18000030517578\n",
      "output: 9.098543167114258  target: 12.0\n",
      "output: 17.91595458984375  target: 19.610000610351562\n",
      "output: 5.920464992523193  target: 5.119999885559082\n",
      "output: 12.848078727722168  target: 13.6850004196167\n",
      "output: 3.6128268241882324  target: 3.5999999046325684\n",
      "output: 1.1052656173706055  target: 2.4800000190734863\n",
      "output: 0.0628499984741211  target: 1.0\n",
      "output: 7.720378875732422  target: 9.100000381469727\n",
      "output: -1.7133855819702148  target: -0.8100000023841858\n",
      "output: 4.7709150314331055  target: 5.539999961853027\n",
      "output: 8.479947090148926  target: 9.399999618530273\n",
      "output: 7.1628618240356445  target: 6.949999809265137\n",
      "output: 6.110726356506348  target: 6.260000228881836\n",
      "output: 13.888792037963867  target: 15.0600004196167\n",
      "output: 0.17135381698608398  target: -0.20000000298023224\n",
      "output: 7.7311506271362305  target: 7.739999771118164\n",
      "output: 3.3465471267700195  target: 1.399999976158142\n",
      "output: 1.7782578468322754  target: 0.6000000238418579\n",
      "output: 17.72620964050293  target: 18.25\n",
      "output: 11.063240051269531  target: 7.900000095367432\n",
      "output: 21.09714126586914  target: 18.5\n",
      "output: 9.760537147521973  target: 8.899999618530273\n",
      "output: 20.122188568115234  target: 19.713333129882812\n",
      "output: 10.701824188232422  target: 9.800000190734863\n",
      "output: 18.52265739440918  target: 19.700000762939453\n",
      "output: 13.872956275939941  target: 13.100000381469727\n",
      "output: 10.794601440429688  target: 11.0\n",
      "output: 10.676004409790039  target: 10.800000190734863\n",
      "output: 21.69521713256836  target: 24.489999771118164\n",
      "output: 15.78228759765625  target: 14.800000190734863\n",
      "output: 20.123241424560547  target: 21.559999465942383\n",
      "output: 10.552570343017578  target: 11.0600004196167\n",
      "output: 18.7232666015625  target: 19.6200008392334\n",
      "output: 14.166524887084961  target: 13.5\n",
      "output: 11.695548057556152  target: 11.270000457763672\n",
      "output: 19.16498565673828  target: 19.350000381469727\n",
      "output: 14.331527709960938  target: 14.300000190734863\n",
      "output: 11.690893173217773  target: 11.220000267028809\n",
      "output: 10.70126724243164  target: 10.850000381469727\n",
      "output: 21.491010665893555  target: 17.200000762939453\n",
      "output: 16.260635375976562  target: 13.399999618530273\n",
      "output: 24.730411529541016  target: 23.889999389648438\n",
      "output: 14.871882438659668  target: 13.5\n",
      "output: 12.041570663452148  target: 13.984999656677246\n",
      "output: 14.224863052368164  target: 13.5\n",
      "output: 17.427204132080078  target: 16.75\n",
      "output: 12.247345924377441  target: 13.899999618530273\n",
      "output: 22.100191116333008  target: 24.325000762939453\n",
      "output: 7.439232349395752  target: 9.0600004196167\n",
      "output: 17.507843017578125  target: 18.290000915527344\n",
      "output: 12.202432632446289  target: 11.699999809265137\n",
      "output: 8.848381042480469  target: 8.819999694824219\n",
      "output: 23.023479461669922  target: 26.024999618530273\n",
      "output: 17.75869369506836  target: 20.200000762939453\n",
      "output: 14.060750961303711  target: 15.199999809265137\n",
      "output: 14.38104248046875  target: 15.300000190734863\n",
      "output: 19.048702239990234  target: 18.690000534057617\n",
      "output: 13.314496994018555  target: 14.199999809265137\n",
      "output: 10.395792007446289  target: 9.720000267028809\n",
      "output: 9.948307037353516  target: 10.5\n",
      "output: 33.734737396240234  target: 36.400001525878906\n",
      "output: 7.464415550231934  target: 7.360000133514404\n",
      "output: 26.26984405517578  target: 26.450000762939453\n",
      "output: 13.314432144165039  target: 14.300000190734863\n",
      "output: 23.439125061035156  target: 23.506000518798828\n",
      "output: 13.388921737670898  target: 13.5\n",
      "output: 21.995773315429688  target: 22.479999542236328\n",
      "output: 7.162162780761719  target: 4.755000114440918\n",
      "output: 10.499452590942383  target: 12.449999809265137\n",
      "output: 29.884714126586914  target: 29.969999313354492\n",
      "output: 20.517385482788086  target: 19.0\n",
      "output: 28.53502082824707  target: 27.479999542236328\n",
      "output: 27.516799926757812  target: 26.979999542236328\n",
      "output: 20.63990020751953  target: 18.899999618530273\n",
      "output: 16.15314483642578  target: 17.0\n",
      "output: 15.761709213256836  target: 15.699999809265137\n",
      "output: 33.39950942993164  target: 33.900001525878906\n",
      "output: 28.11937141418457  target: 30.0\n",
      "output: 30.951997756958008  target: 30.25\n",
      "output: 40.90443801879883  target: 42.70000076293945\n",
      "output: 26.761396408081055  target: 28.420000076293945\n",
      "output: 15.996694564819336  target: 17.5\n",
      "output: 21.430400848388672  target: 20.200000762939453\n",
      "output: 17.45619010925293  target: 17.399999618530273\n",
      "output: 10.111270904541016  target: 10.649999618530273\n",
      "output: 10.103801727294922  target: 10.899999618530273\n",
      "output: 18.810333251953125  target: 18.299999237060547\n",
      "output: 24.541851043701172  target: 25.899999618530273\n",
      "output: 12.247381210327148  target: 13.0\n",
      "output: 11.773637771606445  target: 10.420000076293945\n",
      "output: 11.123798370361328  target: 10.899999618530273\n",
      "output: 19.144283294677734  target: 17.920000076293945\n",
      "output: 6.323740005493164  target: 4.21999979019165\n",
      "output: 14.11732292175293  target: 13.614999771118164\n",
      "output: 8.515928268432617  target: 8.0\n",
      "output: 21.800193786621094  target: 18.329999923706055\n",
      "output: 13.866094589233398  target: 12.600000381469727\n",
      "output: 11.681411743164062  target: 9.989999771118164\n",
      "output: 9.490303039550781  target: 8.5\n",
      "output: 10.215944290161133  target: 9.0\n",
      "output: 20.18671417236328  target: 18.200000762939453\n",
      "output: 12.649635314941406  target: 12.699999809265137\n",
      "output: 10.570711135864258  target: 10.289999961853027\n",
      "output: 11.26837158203125  target: 12.5\n",
      "output: 9.215704917907715  target: 10.675000190734863\n",
      "output: 7.282537460327148  target: 9.0\n",
      "output: 17.778854370117188  target: 18.729999542236328\n",
      "output: 8.998530387878418  target: 10.5\n",
      "output: 18.19184112548828  target: 18.75\n",
      "output: 12.218666076660156  target: 13.0\n",
      "output: 11.026813507080078  target: 10.979999542236328\n",
      "output: 9.883556365966797  target: 10.630000114440918\n",
      "output: 11.713212966918945  target: 13.5\n",
      "output: 9.549522399902344  target: 10.899999618530273\n",
      "output: 18.211076736450195  target: 18.399999618530273\n",
      "output: 21.062532424926758  target: 22.969999313354492\n",
      "output: 5.209039688110352  target: 4.5\n",
      "output: 15.209646224975586  target: 11.539999961853027\n",
      "output: 9.611143112182617  target: 13.899999618530273\n",
      "output: 21.526512145996094  target: 28.735000610351562\n",
      "output: 17.555892944335938  target: 20.139999389648438\n",
      "output: 8.89410400390625  target: 8.199999809265137\n",
      "output: 16.893310546875  target: 15.103333473205566\n",
      "output: 30.385133743286133  target: 21.940000534057617\n",
      "output: 62.5933952331543  target: 56.70000076293945\n",
      "output: 26.014495849609375  target: 26.959999084472656\n",
      "output: 29.30336570739746  target: 24.940000534057617\n",
      "output: 25.359220504760742  target: 20.299999237060547\n",
      "output: 46.27593994140625  target: 45.20000076293945\n",
      "output: 15.075007438659668  target: 11.0\n",
      "output: 15.193145751953125  target: 17.0\n",
      "output: 23.38709259033203  target: 22.77666664123535\n",
      "output: 39.915409088134766  target: 46.099998474121094\n",
      "output: 13.215089797973633  target: 11.699999809265137\n",
      "output: 20.076011657714844  target: 19.0\n",
      "output: 23.344894409179688  target: 25.010000228881836\n",
      "output: 18.784202575683594  target: 23.200000762939453\n",
      "output: 45.88108444213867  target: 56.70000076293945\n",
      "output: 18.72865104675293  target: 21.940000534057617\n",
      "output: 11.179880142211914  target: 13.869999885559082\n",
      "output: 53.57438278198242  target: 57.79999923706055\n",
      "output: 22.132341384887695  target: 22.799999237060547\n",
      "output: 13.109086990356445  target: 13.199999809265137\n",
      "output: 24.59409523010254  target: 26.969999313354492\n",
      "output: 16.299917221069336  target: 12.300000190734863\n",
      "output: 26.33477020263672  target: 24.950000762939453\n",
      "output: 19.66249656677246  target: 20.5\n",
      "output: 5.820643424987793  target: 4.349999904632568\n",
      "output: 9.263944625854492  target: 4.590000152587891\n",
      "output: 21.34537124633789  target: 16.969999313354492\n",
      "output: 31.06672477722168  target: 24.670000076293945\n",
      "output: 23.15380859375  target: 25.799999237060547\n",
      "output: 6.026420593261719  target: 4.869999885559082\n",
      "output: 42.75873565673828  target: 38.0\n",
      "output: 6.833597660064697  target: 5.949999809265137\n",
      "output: 48.974369049072266  target: 43.29999923706055\n",
      "output: 8.910538673400879  target: 10.039999961853027\n",
      "output: 8.032561302185059  target: 11.100000381469727\n",
      "output: 11.679193496704102  target: 11.460000038146973\n",
      "output: 17.07854461669922  target: 17.58333396911621\n",
      "output: 4.034915924072266  target: 4.849999904632568\n",
      "output: 5.291062355041504  target: 5.909999847412109\n",
      "output: 24.140382766723633  target: 27.450000762939453\n",
      "output: 11.506708145141602  target: 16.299999237060547\n",
      "output: 24.0552978515625  target: 25.885000228881836\n",
      "output: 9.31299114227295  target: 10.210000038146973\n",
      "output: 5.499835968017578  target: 5.840000152587891\n",
      "output: 7.908803939819336  target: 9.710000038146973\n",
      "output: 40.778804779052734  target: 43.29999923706055\n",
      "output: 8.625564575195312  target: 10.039999961853027\n",
      "output: 4.353634357452393  target: 3.869999885559082\n",
      "output: 11.217642784118652  target: 11.970000267028809\n",
      "output: 10.999837875366211  target: 11.100000381469727\n",
      "output: 8.295588493347168  target: 8.40999984741211\n",
      "output: 13.644182205200195  target: 13.319999694824219\n",
      "output: 9.356622695922852  target: 10.5\n",
      "output: 25.914873123168945  target: 27.5\n",
      "output: 5.975078582763672  target: 2.700000047683716\n",
      "output: -0.8784704208374023  target: 0.0\n",
      "output: 22.977989196777344  target: 23.899999618530273\n",
      "output: 12.245710372924805  target: 12.699999809265137\n",
      "output: 11.088241577148438  target: 10.5\n",
      "output: 9.755630493164062  target: 9.0\n",
      "output: 18.408960342407227  target: 18.200000762939453\n",
      "output: 18.41579246520996  target: 18.260000228881836\n",
      "output: 12.349761962890625  target: 13.0\n",
      "output: 11.247891426086426  target: 10.649999618530273\n",
      "output: 9.800806045532227  target: 10.699999809265137\n",
      "output: 12.046255111694336  target: 11.0\n",
      "output: 10.788724899291992  target: 10.300000190734863\n",
      "output: 19.26486587524414  target: 18.799999237060547\n",
      "output: 15.075873374938965  target: 13.800000190734863\n",
      "output: 12.875207901000977  target: 10.529999732971191\n",
      "output: 12.313678741455078  target: 10.699999809265137\n",
      "output: 19.970844268798828  target: 18.440000534057617\n",
      "output: 24.187259674072266  target: 22.700000762939453\n",
      "output: 12.225688934326172  target: 12.699999809265137\n",
      "output: 9.955941200256348  target: 10.890000343322754\n",
      "output: 8.935694694519043  target: 8.399999618530273\n",
      "output: 17.83137321472168  target: 18.09000015258789\n",
      "output: 9.10252571105957  target: 10.0\n",
      "output: 18.02028465270996  target: 18.309999465942383\n",
      "output: 12.284561157226562  target: 12.600000381469727\n",
      "output: 9.966976165771484  target: 11.25\n",
      "output: 13.23892593383789  target: 13.399999618530273\n",
      "output: 10.370949745178223  target: 10.59000015258789\n",
      "output: 10.244523048400879  target: 11.100000381469727\n",
      "output: 18.649356842041016  target: 18.260000228881836\n",
      "output: 32.270023345947266  target: 32.939998626708984\n",
      "output: 20.658435821533203  target: 20.0\n",
      "output: 26.350322723388672  target: 25.299999237060547\n",
      "output: 22.003849029541016  target: 21.149999618530273\n",
      "output: 52.16567611694336  target: 47.79999923706055\n",
      "output: 4.188277721405029  target: 3.0\n",
      "output: 49.25167465209961  target: 50.599998474121094\n",
      "output: 14.851622581481934  target: 16.077499389648438\n",
      "output: 4.981661796569824  target: 4.5\n",
      "output: 15.492246627807617  target: 17.389999389648438\n",
      "output: 47.51243209838867  target: 52.20000076293945\n",
      "output: 5.277883529663086  target: 5.889999866485596\n",
      "output: 5.115541934967041  target: 4.900000095367432\n",
      "output: 50.52396774291992  target: 52.5\n",
      "output: 17.715167999267578  target: 17.7450008392334\n",
      "output: 17.420698165893555  target: 18.079999923706055\n",
      "output: 2.98753023147583  target: 3.200000047683716\n",
      "output: 49.540748596191406  target: 49.400001525878906\n",
      "output: 14.910263061523438  target: 14.899999618530273\n",
      "output: 54.15907669067383  target: 52.0\n",
      "output: 18.242706298828125  target: 17.5\n",
      "output: 19.338008880615234  target: 17.385000228881836\n",
      "output: 6.025449752807617  target: 4.900000095367432\n",
      "output: 18.09604263305664  target: 18.075000762939453\n",
      "output: 14.286689758300781  target: 14.895000457763672\n",
      "output: 2.711686134338379  target: 3.200000047683716\n",
      "output: 49.090816497802734  target: 49.400001525878906\n",
      "output: 52.674713134765625  target: 52.0\n",
      "output: 17.32405662536621  target: 17.489999771118164\n",
      "output: 21.47066307067871  target: 22.860000610351562\n",
      "output: 9.618603706359863  target: 10.100000381469727\n",
      "output: 0.0635671615600586  target: 2.359999895095825\n",
      "output: -4.188706874847412  target: -5.900000095367432\n",
      "output: 44.95899963378906  target: 45.20000076293945\n",
      "output: -2.6329684257507324  target: -2.0\n",
      "output: 23.962696075439453  target: 25.850000381469727\n",
      "output: 17.875518798828125  target: 19.700000762939453\n",
      "output: 13.95212173461914  target: 15.100000381469727\n",
      "output: 15.060148239135742  target: 16.399999618530273\n",
      "output: 11.80702018737793  target: 10.640000343322754\n",
      "output: 15.603266716003418  target: 15.300000190734863\n",
      "output: 12.16907024383545  target: 11.100000381469727\n",
      "output: 22.309417724609375  target: 21.260000228881836\n",
      "output: 13.606575012207031  target: 12.699999809265137\n",
      "output: 25.434917449951172  target: 25.125\n",
      "output: 26.77941131591797  target: 23.0\n",
      "output: 8.819955825805664  target: 7.510000228881836\n",
      "output: 0.24551677703857422  target: 0.6000000238418579\n",
      "output: 28.421499252319336  target: 25.469999313354492\n",
      "output: 21.738414764404297  target: 18.0\n",
      "output: 14.384077072143555  target: 15.0\n",
      "output: 14.896385192871094  target: 14.800000190734863\n",
      "output: 25.73910903930664  target: 23.534000396728516\n",
      "output: 9.303670883178711  target: 11.5\n",
      "output: 12.148070335388184  target: 10.350000381469727\n",
      "output: 60.176204681396484  target: 58.099998474121094\n",
      "output: 7.627437591552734  target: 7.639999866485596\n",
      "output: 41.583988189697266  target: 36.099998474121094\n",
      "output: 25.192913055419922  target: 24.479999542236328\n",
      "output: 10.406146049499512  target: 11.859999656677246\n",
      "output: 10.938190460205078  target: 11.600000381469727\n",
      "output: 10.81753158569336  target: 11.600000381469727\n",
      "output: 36.381656646728516  target: 36.099998474121094\n",
      "output: 5.679717063903809  target: 6.159999847412109\n",
      "output: 4.6530232429504395  target: 6.375\n",
      "output: 5.847635269165039  target: 6.864999771118164\n",
      "output: 22.649696350097656  target: 25.09000015258789\n",
      "output: 9.106626510620117  target: 9.930000305175781\n",
      "output: 21.340856552124023  target: 24.030000686645508\n",
      "output: 9.137273788452148  target: 9.970000267028809\n",
      "output: 29.940902709960938  target: 30.0\n",
      "output: 5.658774375915527  target: 6.400000095367432\n",
      "output: 6.573312282562256  target: 6.690000057220459\n",
      "output: 7.0070953369140625  target: 6.5\n",
      "output: 5.421393394470215  target: 5.179999828338623\n",
      "output: 5.626156330108643  target: 5.889999866485596\n",
      "output: 5.127010822296143  target: 4.5\n",
      "output: 5.071468353271484  target: 4.014999866485596\n",
      "output: 29.0383358001709  target: 37.099998474121094\n",
      "output: 5.992264747619629  target: 5.380000114440918\n",
      "output: 6.251324653625488  target: 5.860000133514404\n",
      "output: 11.485723495483398  target: 11.0649995803833\n",
      "output: 31.62019920349121  target: 32.20000076293945\n",
      "output: 7.88095235824585  target: 6.699999809265137\n",
      "output: 7.532192707061768  target: 6.744999885559082\n",
      "output: 11.209522247314453  target: 10.4399995803833\n",
      "output: 41.581321716308594  target: 44.20000076293945\n",
      "output: 11.32927131652832  target: 8.859999656677246\n",
      "output: 37.88050842285156  target: 33.0\n",
      "output: 8.618644714355469  target: 8.399999618530273\n",
      "output: 37.83503723144531  target: 34.29999923706055\n",
      "output: 8.675111770629883  target: 7.559999942779541\n",
      "output: 5.1253509521484375  target: 4.900000095367432\n",
      "output: 54.06132125854492  target: 52.29999923706055\n",
      "output: 18.286508560180664  target: 18.137500762939453\n",
      "output: 4.268667221069336  target: 3.9000000953674316\n",
      "output: -0.5049996376037598  target: -1.3899999856948853\n",
      "output: 52.94444274902344  target: 54.70000076293945\n",
      "output: 6.087860107421875  target: 7.5\n",
      "output: 19.66556167602539  target: 20.360000610351562\n",
      "output: 0.7154550552368164  target: 1.7999999523162842\n",
      "output: 6.700553894042969  target: 9.5\n",
      "output: 53.157962799072266  target: 55.70000076293945\n",
      "output: 6.987593173980713  target: 7.974999904632568\n",
      "output: 20.73981285095215  target: 21.09600067138672\n",
      "output: 7.311570644378662  target: 6.400000095367432\n",
      "output: 7.565457344055176  target: 4.900000095367432\n",
      "output: 54.11475372314453  target: 52.29999923706055\n",
      "output: 20.08685302734375  target: 18.135000228881836\n",
      "output: 20.480266571044922  target: 26.149999618530273\n",
      "output: 7.8343095779418945  target: 12.800000190734863\n",
      "output: 17.752206802368164  target: 18.8799991607666\n",
      "output: 8.108652114868164  target: 14.800000190734863\n",
      "output: 20.80966567993164  target: 28.125\n",
      "output: 35.26042938232422  target: 47.5\n",
      "output: 13.737857818603516  target: 12.979999542236328\n",
      "output: 27.363401412963867  target: 21.104999542236328\n",
      "output: 64.37753295898438  target: 55.70000076293945\n",
      "output: 10.40176773071289  target: 4.400000095367432\n",
      "output: 9.952390670776367  target: 3.299999952316284\n",
      "output: 20.44365692138672  target: 16.125\n",
      "output: 17.604740142822266  target: 16.014999389648438\n",
      "output: 12.7459716796875  target: 10.649999618530273\n",
      "output: 6.9451446533203125  target: 5.380000114440918\n",
      "output: 6.914858818054199  target: 6.260000228881836\n",
      "output: 14.873994827270508  target: 16.395000457763672\n",
      "output: 47.82246017456055  target: 50.5\n",
      "output: 3.5117616653442383  target: 2.799999952316284\n",
      "output: -10.622274398803711  target: -9.5\n",
      "output: 38.47556686401367  target: 37.70000076293945\n",
      "output: -8.682894706726074  target: -10.899999618530273\n",
      "output: 33.11556625366211  target: 33.5\n",
      "output: 6.599689483642578  target: 1.440000057220459\n",
      "output: 9.822338104248047  target: 10.170000076293945\n",
      "output: 39.6432991027832  target: 38.0\n",
      "output: 7.381877422332764  target: 3.75\n",
      "output: 16.81892204284668  target: 19.65999984741211\n",
      "output: 6.789126873016357  target: 7.800000190734863\n",
      "output: 9.304952621459961  target: 12.479999542236328\n",
      "output: 3.658329963684082  target: 4.539999961853027\n",
      "output: 30.08645248413086  target: 35.900001525878906\n",
      "output: -7.544827461242676  target: -2.0\n",
      "output: 41.048770904541016  target: 38.400001525878906\n",
      "output: 4.868666648864746  target: 4.619999885559082\n",
      "output: 7.75327205657959  target: 8.154999732971191\n",
      "output: 7.228602886199951  target: 7.954999923706055\n",
      "output: 42.42353820800781  target: 39.0\n",
      "output: 6.888560771942139  target: 5.375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b1e428e5ecd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlosslist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "n_features = 300\n",
    "n_hidden = 100\n",
    "losslist = []\n",
    "for t in range(epochs):\n",
    "    for b in range(len(sol_data)):\n",
    "        solute = sol_data[b]\n",
    "        solvent = solv_data[b]\n",
    "        target = targets[b]  \n",
    "        output = dmodel(solute,solvent) \n",
    "        loss = criterion(output, target)  \n",
    "        print(\"output:\",output.item(),\" target:\",target.item())\n",
    "        losslist.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print('step : ' , t , 'loss : ' , loss.item())\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         526 function calls (518 primitive calls) in 0.078 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.078    0.078 {built-in method builtins.exec}\n",
      "        1    0.005    0.005    0.078    0.078 <string>:1(<module>)\n",
      "      9/1    0.000    0.000    0.073    0.073 module.py:715(_call_impl)\n",
      "        1    0.000    0.000    0.073    0.073 <ipython-input-208-c9036915f170>:23(forward)\n",
      "        2    0.000    0.000    0.070    0.035 rnn.py:555(forward)\n",
      "        2    0.069    0.035    0.069    0.035 {built-in method lstm}\n",
      "        2    0.000    0.000    0.002    0.001 <ipython-input-207-127204d06c41>:8(att)\n",
      "        2    0.001    0.000    0.001    0.001 <ipython-input-207-127204d06c41>:1(alpha)\n",
      "        2    0.001    0.000    0.001    0.000 {built-in method exp}\n",
      "        2    0.000    0.000    0.000    0.000 <ipython-input-208-c9036915f170>:10(forward)\n",
      "        2    0.000    0.000    0.000    0.000 pooling.py:152(forward)\n",
      "        2    0.000    0.000    0.000    0.000 _jit_internal.py:257(fn)\n",
      "        2    0.000    0.000    0.000    0.000 functional.py:574(_max_pool2d)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method max_pool2d}\n",
      "        2    0.000    0.000    0.000    0.000 linear.py:92(forward)\n",
      "        2    0.000    0.000    0.000    0.000 functional.py:1669(linear)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'matmul' of 'torch._C._TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 <ipython-input-208-c9036915f170>:7(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method cat}\n",
      "        4    0.000    0.000    0.000    0.000 module.py:223(__init__)\n",
      "       54    0.000    0.000    0.000    0.000 module.py:781(__setattr__)\n",
      "        2    0.000    0.000    0.000    0.000 pooling.py:17(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method zeros}\n",
      "        2    0.000    0.000    0.000    0.000 rnn.py:529(check_forward_args)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method pow}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method sum}\n",
      "        2    0.000    0.000    0.000    0.000 rnn.py:171(check_input)\n",
      "        1    0.000    0.000    0.000    0.000 functional.py:1124(relu)\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method t}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method relu}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "      112    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        2    0.000    0.000    0.000    0.000 {method 't' of 'torch._C._TensorBase' objects}\n",
      "      160    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "       10    0.000    0.000    0.000    0.000 module.py:765(__getattr__)\n",
      "       10    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 overrides.py:1070(has_torch_function)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "       36    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "        2    0.000    0.000    0.000    0.000 _VF.py:25(__getattr__)\n",
      "        4    0.000    0.000    0.000    0.000 rnn.py:193(check_hidden_size)\n",
      "        6    0.000    0.000    0.000    0.000 overrides.py:1083(<genexpr>)\n",
      "       20    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        2    0.000    0.000    0.000    0.000 rnn.py:182(get_expected_hidden_size)\n",
      "        2    0.000    0.000    0.000    0.000 functional.py:1686(<listcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        2    0.000    0.000    0.000    0.000 module.py:782(remove_from)\n",
      "        2    0.000    0.000    0.000    0.000 rnn.py:538(permute_hidden)\n",
      "        5    0.000    0.000    0.000    0.000 _jit_internal.py:750(is_scripting)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch._C._is_torch_function_enabled}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run(\"dmodel(X[2],Y[0])\", sort = \"cumtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolvDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.landmarks_frame.iloc[idx, 0]\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_IDs, solute, solvent, labels):\n",
    "        self.labels = labels\n",
    "        self.solute = solute\n",
    "        self.solvent = solvent\n",
    "        self.list_IDs = list_IDs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ID = self.list_IDs[index]\n",
    "        \n",
    "        X = (self.solute[ID],self.solvent[ID])\n",
    "        y = self.labels[ID]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([11, 300]), torch.Size([31, 300]), torch.Size([19, 300]), torch.Size([8, 300]), torch.Size([8, 300])]\n",
      "torch.Size([11, 2, 300]) torch.Size([2]) torch.Size([11, 2, 300])\n",
      "tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ..., False, False, False]]])\n",
      "torch.Size([31, 2, 300]) torch.Size([2]) torch.Size([31, 2, 300])\n",
      "tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ..., False, False, False]]])\n",
      "torch.Size([19, 1, 300]) torch.Size([1]) torch.Size([19, 1, 300])\n",
      "tensor([[[True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "sample = [torch.Tensor(x) for x in sol_data[0:5]]\n",
    "print([t.shape for t in sample])\n",
    "loader = DataLoader(sample, batch_size=2, shuffle=True, collate_fn=collate_fn_padd)\n",
    "\n",
    "for batch_idx, (x, t, m) in enumerate(loader):\n",
    "    print(x.shape, t.shape, m.shape)\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    Padds batch of variable length\n",
    "    '''\n",
    "    ## get sequence lengths\n",
    "    lengths = torch.tensor([t.shape[0] for t in batch])\n",
    "    ## padd\n",
    "    batch = [torch.Tensor(t) for t in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch)\n",
    "    ## compute mask\n",
    "    mask = (batch != 0)\n",
    "    return batch, lengths, mask\n",
    "\n",
    "def collate_double(batch):\n",
    "    '''\n",
    "    Padds batch of variable length\n",
    "    '''\n",
    "    sol_batch = [torch.Tensor(t[0][0]) for t in batch]\n",
    "    sol_batch = torch.nn.utils.rnn.pad_sequence(sol_batch)\n",
    "    solv_batch = [torch.Tensor(t[0][1]) for t in batch]\n",
    "    solv_batch = torch.nn.utils.rnn.pad_sequence(solv_batch)\n",
    "    targets = torch.Tensor([t[1].item() for t in batch])\n",
    "    \n",
    "    return [sol_batch, solv_batch, targets]\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn_padd)\n",
    "\n",
    "#for batch_idx, (x, t) in enumerate(loader):\n",
    "#   print(x.shape, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch model definition\n",
    "epochs = 10\n",
    "n_features = 300\n",
    "n_hidden = 100\n",
    "batch_size = 32\n",
    "\n",
    "#attention alignment (alpha) of H wrt G\n",
    "def alpha(G,H):\n",
    "    alpha = torch.exp(H@torch.t(G))\n",
    "    norm = torch.sum(alpha, dim=1)\n",
    "    norm = torch.pow(norm, -1)\n",
    "    alpha = alpha * norm[:, None]\n",
    "    return alpha\n",
    "\n",
    "#inH = H;P, where P is the emphasised hidden state H / solvent context\n",
    "def att(G,H):\n",
    "    P = alpha(G,H)@G\n",
    "#    inH = torch.cat((H,P),1)\n",
    "    inH = torch.stack((H,P),2)\n",
    "    return inH\n",
    "\n",
    "#module for variable maxpooling with interaction vectors\n",
    "class maxpool_int(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(maxpool, self).__init__()\n",
    "        self.maxpool = nn.MaxPool3d((L,1,2))\n",
    "    def forward(self, X):\n",
    "        return self.maxpool(X)\n",
    "\n",
    "#module for variable maxpooling without interaction vectors\n",
    "class maxpool(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(maxpool, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d((L,1))\n",
    "    def forward(self, X):\n",
    "        return self.maxpool(X)\n",
    "\n",
    "#delfos model\n",
    "class dnet(nn.Module):\n",
    "    def __init__(self, n_features=300, D=150, FF=2000, interaction=True):\n",
    "        super(dnet, self).__init__()\n",
    "        self.features = n_features\n",
    "        self.dim = D\n",
    "        self.hidden = FF\n",
    "        self.interaction = interaction\n",
    "    \n",
    "        self.biLSTM_X = nn.LSTM(self.features, self.dim, bidirectional=True)\n",
    "        self.biLSTM_Y = nn.LSTM(self.features, self.dim, bidirectional=True)\n",
    "        \n",
    "        self.FF = nn.Linear(4*self.dim, self.hidden)\n",
    "        self.out = nn.Linear(self.hidden, 1)\n",
    "    \n",
    "    def forward(self,Y,X):\n",
    "        #max sequence lengths\n",
    "        N = X.shape[0] #solvent\n",
    "        M = Y.shape[0] #solute\n",
    "        #batch size\n",
    "        B = X.shape[1] \n",
    "        \n",
    "        #biLSTM to get hidden states\n",
    "        H, hcX = self.biLSTM_X(X, None) #NxBx2D tensor - solvent hidden state\n",
    "        G, hcY = self.biLSTM_Y(Y, None) #MxBx2D tensor - solute hidden state\n",
    "        \n",
    "        if self.interaction:\n",
    "            #calculate attention, then concatenate with hidden states H and G\n",
    "            inH = torch.stack([att(G[:,b,:],H[:,b,:]) for b in range(B)],0) #BxNx2Dx2\n",
    "            inG = torch.stack([att(H[:,b,:],G[:,b,:]) for b in range(B)],0) #BxMx2Dx2\n",
    "        \n",
    "            #maxpool concatenated tensors\n",
    "            maxpool_X = maxpool_int(N)\n",
    "            maxpool_Y = maxpool_int(M)\n",
    "            u = maxpool_X(inH).view(B,2*self.dim)  #Bx2D - solvent vector\n",
    "            v = maxpool_Y(inG).view(B,2*self.dim)  #Bx2D - solute vector\n",
    "        \n",
    "        else:\n",
    "            #maxpool tensors\n",
    "            maxpool_X = maxpool(N)\n",
    "            maxpool_Y = maxpool(M)\n",
    "            u = maxpool_X(torch.transpose(H,0,1)).view(B,2*self.dim)  #Bx2D - solvent vector\n",
    "            v = maxpool_Y(torch.transpose(G,0,1)).view(B,2*self.dim)  #Bx2D - solute vector\n",
    "        \n",
    "        #feed forward neural network\n",
    "        NN = torch.cat((u,v),1) #Bx4D - concatenated solvent/solute vector\n",
    "        NN = self.FF(NN) #Bxhidden\n",
    "        NN = nn.functional.relu(NN)\n",
    "        output = self.out(NN).view(B)\n",
    "        return output\n",
    "\n",
    "dmodel = dnet(300,100,2000,False)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(dmodel.parameters(), lr=0.0002, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0044, -0.0069], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmodel(sample[0],sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0 loss :  27.570920944213867\n",
      "step :  1 loss :  0.7333709001541138\n",
      "step :  2 loss :  2.6808462142944336\n",
      "step :  3 loss :  3.8494668006896973\n",
      "step :  4 loss :  6.193925857543945\n",
      "step :  5 loss :  15.363566398620605\n",
      "step :  6 loss :  3.488748073577881\n",
      "step :  7 loss :  2.0179247856140137\n",
      "step :  8 loss :  9.508049011230469\n",
      "step :  9 loss :  8.998316764831543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x138013490>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset([x for x in range(len(sol_data))], sol_data, solv_data, targets)\n",
    "loader = DataLoader(dataset, batch_size=6, shuffle=True, collate_fn=collate_double)\n",
    "\n",
    "sample = iter(loader)\n",
    "\n",
    "for batch_idx, (sol,solv,t) in enumerate(loader):\n",
    "    sample = (sol,solv,t)\n",
    "    #apply training to batches!\n",
    "    \n",
    "    \n",
    "epochs = 10\n",
    "n_features = 300\n",
    "n_hidden = 100\n",
    "losslist = []\n",
    "for x in range(epochs):\n",
    "    for (sol,solv,t) in loader:\n",
    "        output = dmodel(sol,solv) \n",
    "        loss = criterion(output, t)  \n",
    "        losslist.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print('step : ' , x , 'loss : ' , loss.item())\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-ac9490f567b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0msolute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msolvent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    for b in range(len(X)):\n",
    "        solute = X[b]\n",
    "        solvent = Y[0]\n",
    "        target = targets[b]  \n",
    "\n",
    "        output = dmodel(solute,solvent) \n",
    "        print(output)\n",
    "        loss = criterion(output, target)  \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad() \n",
    "    print('step : ' , t , 'loss : ' , loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
